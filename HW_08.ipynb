{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 8**\n",
    "\n",
    "Due: **November 26, 5pm** (late submission until November 29, 5pm -- no submission possible afterwards)\n",
    "\n",
    "Written assigment: 20 points\n",
    "\n",
    "Coding assignment: 25 points\n",
    "\n",
    "Project report: 10 points\n",
    "\n",
    "### Name: [TODO]\n",
    "\n",
    "### Link to the github repo: [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Written Assignment**\n",
    "\n",
    "### **Neural Networks**\n",
    "Here, we consider a 2-layer neural network that takes inputs of\n",
    "dimension $d$, has a hidden layer of size $m$, and produces scalar\n",
    "outputs.\n",
    "\n",
    "The network's parameters are $W$, $\\mathbf{b_1}$, $\\mathbf{v}$, and $b_2$. $W$\n",
    "is a $m \\times d$ matrix, $\\mathbf{b_1}$ is an $m$-dimensional vector,\n",
    "$\\mathbf{v}$ is an $m$-dimensional vector, and $b_2$ is a scalar. <br>\n",
    "For an input $\\mathbf{x}$, the output of the first layer of the network is:\n",
    "$$\\mathbf{h} = \\sigma(W\\mathbf{x}+\\mathbf{b_1})$$ \n",
    "and the output of the second layer is: $$z = \\mathbf{v}\\cdot \\mathbf{h} + b_2,$$ where $\\sigma$ is an activation\n",
    "function. For this question, let $\\sigma$ be the sigmoid activation\n",
    "function $\\sigma_{\\text{sigmoid}}$ (in the formula below, we apply it\n",
    "element-wise): $$\\sigma_{\\text{sigmoid}}(a) = \\frac{1}{1+e^{-a}}$$ We\n",
    "will be using the following loss function: $$L(z) = (z - y)^2,$$ where\n",
    "$y$ is a real-valued label and $z$ is the network's output. <br>\n",
    "In this problem, you will calculate the partial derivative of $L(z)$\n",
    "with respect to each of the network's parameters. Let $w_{ij}$ be the\n",
    "entry at the $i^{th}$ row and $j^{th}$ column of $W$. Let $v_i$ be the $i^{th}$\n",
    "component of $\\mathbf{v}$. Let $b_{1i}$ be the $i$th component of\n",
    "$\\mathbf{b_1}$. (Note that $1 \\leq i \\leq m$ and $1 \\leq j \\leq d$.) <br>\n",
    "(Hint: For each part of the problem, apply the chain rule.)\n",
    "\n",
    "### **Question 1**\n",
    "Calculate $\\frac{\\partial{L(z)}}{\\partial{b_2}}$. Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "Calculate $\\frac{\\partial{L(z)}}{\\partial{v_i}}$. Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 3**\n",
    "Calculate $\\frac{\\partial{L(z)}}{\\partial{b_{1i}}}$. Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 4**\n",
    "Calculate $\\frac{\\partial{L(z)}}{\\partial{w_{ij}}}$. Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Programming Assignment**\n",
    "\n",
    "### Introduction \n",
    "\n",
    "In this assignment, you will be implementing feed forward neural\n",
    "networks using stochastic gradient descent. You will implement two\n",
    "neural networks: a single layer neural network and a two-layer neural\n",
    "network. You will compare the performance of both models on the UCI Wine\n",
    "Dataset, which you previously used in HW2. The task is to predict the\n",
    "quality of a wine (scored out of 10) given various attributes of the\n",
    "wine (for example, acidity, alcohol content). The book section relevant\n",
    "to this assignment is 20.1.\n",
    "\n",
    "### Stencil Code & Data\n",
    "\n",
    "We have provided the following stencil code:\n",
    "\n",
    "-   `Models` contains the `OneLayerNN` model and the `TwoLayerNN`\n",
    "    model which you will be implementing.\n",
    "\n",
    "-   `Check Model` contains a series of tests to ensure you are coding your \n",
    "    model properly.\n",
    "\n",
    "-   `Main` is the entry point of program which will read in the\n",
    "    dataset, run the models and print the results.\n",
    "\n",
    "You should not need to add any code to `Main`. If you do for\n",
    "debugging or other purposes, please make sure all of your additions are\n",
    "commented out in the final handin. All the functions you need to fill in\n",
    "reside in `Models`, marked by `TODO`s.\n",
    "\n",
    "We have provided unit tests for the functions that compute gradients for\n",
    "the 2-layer neural network. These are `_layer1_weights_gradient`,\n",
    "`_layer1_bias_gradient`, `_layer2_weights_gradient`, and\n",
    "`_layer2_bias_gradient`. To enable these unit tests, uncomment\n",
    "`test_gradients` in the `main` function of `main.py`.\n",
    "\n",
    "Your program assumes the data is formatted as follows: The first column\n",
    "of data in each file is the dependent variable (the observations $y$)\n",
    "and all other columns are the independent input variables\n",
    "($x_{1}, x_{2}, \\ldots, x_{n}$). We have taken care of all data\n",
    "preprocessing, as usual.\n",
    "\n",
    "If you're curious and would like to read about the dataset, you can find\n",
    "more information [here](https://archive.ics.uci.edu/ml/datasets/wine), but it is\n",
    "strongly recommended that you use the versions that we've provided in\n",
    "the course directory to maintain consistent formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Assignment**\n",
    "\n",
    "### **Neural Networks**\n",
    "\n",
    "For this assignment, we will be evaluating each model using total\n",
    "squared loss (or L2 loss). Recall that the L2 loss function is defined\n",
    "as:\n",
    "\n",
    "$L(h) = \\sum\\limits_{i=1}^m \\ell(h({\\bf x}_i), y_i) = \\sum\\limits_{i=1}^m(y_{i}-h({\\bf x}_{i}))^{2}$\n",
    "\n",
    "where $y_{i}$ is the target value of $i^{th}$ sample and\n",
    "$h({\\bf x}_{i})$ is the predicted value given the learned model weights.\n",
    "Each of the two models will use stochastic gradient descent to minimize\n",
    "this loss function.\n",
    "\n",
    "For this assignment, you will be implementing two models:\n",
    "\n",
    "-   `OneLayerNN`: The one-layer neural network is an equivalent model to\n",
    "    Linear Regression. It also learns linear functions of the inputs:\n",
    "    $$h({\\bf x}) = \\langle {\\bf w},  {\\bf x} \\rangle + b.$$\n",
    "\n",
    "    Therefore, when using squared loss, the ERM hypothesis has weights\n",
    "    $${\\bf w} = \\text{argmin}_{{\\bf w}} \\sum_{i = 1}^{m}(y_{i} - h({\\bf x}_i))^{2} .$$\n",
    "\n",
    "    To find the optimal set of weights, you should use Stochastic\n",
    "    Gradient Descent. *Hint:* Compute the derivative of the loss with\n",
    "    respect to ${\\bf w}$. Then, use the SGD algorithm to minimize the\n",
    "    loss.\n",
    "\n",
    "-   `TwoLayerNN`: For this model, you will be implement a neural network\n",
    "    with a fully connected hidden layer.\n",
    "\n",
    "    For an input ${\\bf x}$, the output of the first layer of the network\n",
    "    is $${\\bf v} = \\sigma(W_1 {\\bf x}+{\\bf b}_1)$$ and the output of the\n",
    "    second layer is $$h = \\langle {\\bf w}_2, {\\bf v} \\rangle + b_2 ~.$$\n",
    "\n",
    "    $\\sigma$ is an activation function. In your implementation, you will\n",
    "    take in the activation function $\\sigma(a)$ as a parameter to\n",
    "    $\\texttt{TwoLayerNN}$. Additionally, you will need to pass in the\n",
    "    derivative of the activation function $\\sigma'(a)$ for training.\n",
    "    Doing so will allow you to easily swap out the sigmoid activation\n",
    "    function with other activation functions, such as ReLU. (You can\n",
    "    explore other activation functions for extra credit.)\n",
    "\n",
    "    To complete this assignment, however, you only need to train the\n",
    "    network with the sigmoid activation function. Recall that the\n",
    "    sigmoid activation function is (in the above formula, we apply it\n",
    "    element-wise),\n",
    "    $$\\sigma_{\\text{sigmoid}}(a) = \\frac{1}{1+e^{-a}} .$$\n",
    "     \n",
    "\n",
    "### **Training Neural Networks**\n",
    "\n",
    "The primary objective of training a neural network is to find a set of\n",
    "weights and biases that minimize the loss of our network, which in this\n",
    "case, is L2 loss. If these weights are all initialized to the same\n",
    "constant value, then they will all learn the same features. To avoid\n",
    "this, be sure that your implementation randomly initializes the weights.\n",
    "Numpy functions such as `np.random.normal` or `np.random.uniform` may be\n",
    "useful.\n",
    "\n",
    "When training the two layer neural network, first calculate the\n",
    "gradients of the weights and biases for both layers before updating\n",
    "them. In the stencil, we have given you four methods for computing\n",
    "gradients: `_layer1_weights_gradient`, `_layer1_bias_gradient`,\n",
    "`_layer2_weights_gradient`, and `_layer2_bias_gradient`. Each of these\n",
    "methods should be called before performing gradient descent, i.e. before\n",
    "updating all of the gradients.\n",
    "\n",
    "We also expect that you implement backpropagation as outlined in lecture\n",
    "i.e. computing all the outputs in the forward pass and saving them for\n",
    "use in the backward pass so that backpropagation achieves O(E)\n",
    "complexity, where E represents the number of edges in the network.\n",
    "\n",
    "### **Computing Gradients**\n",
    "\n",
    "Please refer to Lecture 17, slide 34 (Backpropagation) for the\n",
    "definition of the gradient computations.\n",
    "\n",
    "Remember that $\\sigma_1$ and $\\sigma_2$ are the activation functions at\n",
    "each layer, and not necessarily the same! Keep in mind that in your\n",
    "implementation for this assignment there should be no activation applied\n",
    "to the output layer of the network.\n",
    "\n",
    "Finally, note that the initial input matrix is comprised of rows, but\n",
    "the input to each gradient function is a vector. This is due to\n",
    "contradicting conventions about how to represent training data and\n",
    "neural network inputs. To resolve this, you may choose to reshape or\n",
    "transpose the input matrix somewhere in the train method.\n",
    "\n",
    "**Important Note:** External libraries that make the implementation\n",
    "trivial are prohibited. Specifically, `numpy.linalg.lstsqnp` (and\n",
    "similar functions) cannot be used in your implementation. Additionally,\n",
    "you **cannot** use Tensorflow or other neural network libraries. You\n",
    "should implement the neural networks using only Python and Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the evironment test below, make sure you get all green checks. If not, you will lose 2 points for each red or missing sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.11\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.11\"):\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.10.5\", 'numpy': \"2.3.2\",'sklearn': \"1.7.1\", \n",
    "                'pandas': \"2.3.2\", 'pytest': \"8.4.1\", 'torch':\"2.7.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def l2_loss(predictions,Y):\n",
    "    '''\n",
    "        Computes L2 loss (sum squared loss) between true values, Y, and predictions.\n",
    "        :param Y: A 1D Numpy array with real values (float64)\n",
    "        :param predictions: A 1D Numpy array of the same size of Y\n",
    "        :return: L2 loss using predictions for Y.\n",
    "    '''\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "        Sigmoid function f(x) =  1/(1 + exp(-x))\n",
    "        :param x: A scalar or Numpy array\n",
    "        :return: Sigmoid function evaluated at x (applied element-wise if it is an array)\n",
    "    '''\n",
    "    return np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (np.exp(x) + np.exp(0)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    '''\n",
    "        First derivative of the sigmoid function with respect to x.\n",
    "        :param x: A scalar or Numpy array\n",
    "        :return: Derivative of sigmoid evaluated at x (applied element-wise if it is an array)\n",
    "    '''\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "class OneLayerNN:\n",
    "    '''\n",
    "        One layer neural network trained with Stocastic Gradient Descent (SGD)\n",
    "    '''\n",
    "    def __init__(self, batch_size=1):\n",
    "        '''\n",
    "        @attrs:\n",
    "            weights: The weights of the neural network model.\n",
    "            batch_size: The number of examples in each batch\n",
    "            learning_rate: The learning rate to use for SGD\n",
    "            epochs: The number of times to pass through the dataset\n",
    "            v: The resulting predictions computed during the forward pass\n",
    "        '''\n",
    "        # initialize self.weights in train()\n",
    "        self.weights = None\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = 0.001\n",
    "        self.epochs = 25\n",
    "\n",
    "        # initialize self.v in forward_pass()\n",
    "        self.v = None\n",
    "\n",
    "    def train(self, X, Y, print_loss=True):\n",
    "        '''\n",
    "        Trains the OneLayerNN model using SGD.\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :param print_loss: If True, print the loss after each epoch.\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: initialize weights\n",
    "        # HINT: for best performance initialize weights with np.random.uniform in the range [0, 1]\n",
    "\n",
    "        # TODO: Train network for certain number of epochs\n",
    "\n",
    "        # TODO: Shuffle the examples (X) and labels (Y)\n",
    "\n",
    "        # TODO: We need to iterate over each data point for each epoch\n",
    "        # iterate through the examples in batch size increments\n",
    "\n",
    "        # TODO: Perform the forward and backward pass on the current batch using forward_pass() and backward_pass() method\n",
    "\n",
    "        # Print the loss after every epoch\n",
    "        if print_loss:\n",
    "            print('Epoch: {} | Loss: {}'.format(epoch, self.loss(X, Y)))\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        '''\n",
    "        Computes the predictions for a single layer given examples X and\n",
    "        stores them in self.v\n",
    "        :param X: 2D Numpy array where each row contains an example.\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO:\n",
    "        pass\n",
    "\n",
    "    def backward_pass(self, X, Y):\n",
    "        '''\n",
    "        Computes the weights gradient and updates self.weights\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: Compute the gradients for the model's weights using backprop() method\n",
    "\n",
    "        # TODO: Update the weights using gradient_descent() method\n",
    "        pass\n",
    "\n",
    "    def backprop(self, X, Y):\n",
    "        '''\n",
    "        Returns the average weights gradient for the given batch\n",
    "        :param X: 2D Numpy array where each row contains an example.\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A 1D Numpy array representing the weights gradient\n",
    "        '''\n",
    "        # TODO: Compute the average weights gradient\n",
    "        # Refer to the SGD algorithm in slide\n",
    "        pass\n",
    "\n",
    "    def gradient_descent(self, grad_W):\n",
    "        '''\n",
    "        Updates the weights using the given gradient\n",
    "        :param grad_W: A 1D Numpy array representing the weights gradient\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: Update the weights using the given gradient and the learning rate\n",
    "        # Refer to the SGD algorithm in slide\n",
    "        pass\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the total squared error on some dataset (X, Y).\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A float which is the squared error of the model on the dataset\n",
    "        '''\n",
    "        # Perform the forward pass and compute the l2 loss\n",
    "        self.forward_pass(X)\n",
    "        return l2_loss(self.v, Y)\n",
    "\n",
    "    def average_loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the mean squared error on some dataset (X, Y).\n",
    "        MSE = Total squared error/# of examples\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A float which is the mean squared error of the model on the dataset\n",
    "        '''\n",
    "        return self.loss(X, Y) / X.shape[0]\n",
    "\n",
    "class TwoLayerNN:\n",
    "\n",
    "    def __init__(self, hidden_size, batch_size=1,\n",
    "                 activation=sigmoid, activation_derivative=sigmoid_derivative):\n",
    "        '''\n",
    "        @attrs:\n",
    "            activation: the activation function applied after the first layer\n",
    "            activation_derivative: the derivative of the activation function. Used for training.\n",
    "            hidden_size: The hidden size of the network (an integer)\n",
    "            batch_size: The number of examples in each batch\n",
    "            learning_rate: The learning rate to use for SGD\n",
    "            epochs: The number of times to pass through the dataset\n",
    "            wh: The first (hidden) layer weights of the neural network model.\n",
    "            bh: The first (hidden) layer bias of the neural network model.\n",
    "            wout: The second (output) layer weights of the neural network model.\n",
    "            bout: The second (output) layer bias of the neural network model.\n",
    "            a1: The output of the first layer computed during the forward pass\n",
    "            v1: The activated output of the first layer computed during the forward pass\n",
    "            a2: The output of the second layer computed during the forward pass\n",
    "            v2: The resulting predictions computed during the forward pass (layer 2 has the identity activation function)\n",
    "            output_neurons: The number of outputs of the network\n",
    "        '''\n",
    "        self.activation = activation\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = 0.01\n",
    "        self.epochs = 25\n",
    "\n",
    "        # initialize the following weights and biases in the train() method\n",
    "        self.wh = None\n",
    "        self.bh = None\n",
    "        self.wout = None\n",
    "        self.bout = None\n",
    "\n",
    "        # initialize the following values in the forward_pass() method\n",
    "        # these values will be stored and used for the backward_pass()\n",
    "        # note that you may not need to use them all in backward_pass()\n",
    "        self.a1 = None\n",
    "        self.v1 = None\n",
    "        self.a2 = None\n",
    "        self.v2 = None\n",
    "\n",
    "\n",
    "        # In this assignment, we will only use output_neurons = 1.\n",
    "        self.output_neurons = 1\n",
    "\n",
    "    def _get_layer2_bias_gradient(self, x, y):\n",
    "        '''\n",
    "        Computes the gradient of the loss with respect to the output bias, bout.\n",
    "        :param x: Numpy array for a single training example with dimension: input_size by 1\n",
    "        :param y: Label for the training example\n",
    "        :return: the partial derivates dL/dbout, a numpy array of dimension: output_neurons by 1\n",
    "        '''\n",
    "        # TODO:\n",
    "        pass\n",
    "\n",
    "    def _get_layer2_weights_gradient(self, x, y):\n",
    "        '''\n",
    "        Computes the gradient of the loss with respect to the output weights, wout.\n",
    "        :param x: Numpy array for a single training example with dimension: input_size by 1\n",
    "        :param y: Label for the training example\n",
    "        :return: the partial derivates dL/dwout, a numpy array of dimension: output_neurons by hidden_size\n",
    "        '''\n",
    "        # TODO:\n",
    "        pass\n",
    "\n",
    "    def _get_layer1_bias_gradient(self, x, y):\n",
    "        '''\n",
    "        Computes the gradient of the loss with respect to the hidden bias, bh.\n",
    "        :param x: Numpy array for a single training example with dimension: input_size by 1\n",
    "        :param y: Label for the training example\n",
    "        :return: the partial derivates dL/dbh, a numpy array of dimension: hidden_size by 1\n",
    "        '''\n",
    "        # TODO:\n",
    "        pass\n",
    "\n",
    "    def _get_layer1_weights_gradient(self, x, y):\n",
    "        '''\n",
    "        Computes the gradient of the loss with respect to the hidden weights, wh.\n",
    "        :param x: Numpy array for a single training example with dimension: input_size by 1\n",
    "        :param y: Label for the training example\n",
    "        :return: the partial derivates dL/dwh, a numpy array of dimension: hidden_size by input_size\n",
    "        '''\n",
    "        # TODO:\n",
    "        pass\n",
    "\n",
    "    def train(self, X, Y, print_loss=True):\n",
    "        '''\n",
    "        Trains the TwoLayerNN with SGD using Backpropagation.\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :param learning_rate: The learning rate to use for SGD\n",
    "        :param epochs: The number of times to pass through the dataset\n",
    "        :param print_loss: If True, print the loss after each epoch.\n",
    "        :return: None\n",
    "        '''\n",
    "        # NOTE:\n",
    "        # Use numpy arrays of the following dimensions for your model's parameters.\n",
    "        # layer 1 weights (wh): hidden_size x input_size\n",
    "        # layer 1 bias (bh): hidden_size x 1\n",
    "        # layer 2 weights (wout): output_neurons x hidden_size\n",
    "        # layer 2 bias (bout): output_neurons x 1\n",
    "        # HINT: for best performance initialize weights with np.random.uniform in the range [0, 1]\n",
    "\n",
    "        # TODO: Weight and bias initialization\n",
    "\n",
    "        # TODO: Train network for certain number of epochs\n",
    "\n",
    "        # TODO: Shuffle the examples (X) and labels (Y)\n",
    "\n",
    "        # TODO: We need to iterate over each data point for each epoch\n",
    "        # iterate through the examples in batch size increments\n",
    "\n",
    "        # TODO: Perform the forward and backward pass on the current batch using forward_pass() and backward_pass() method\n",
    "\n",
    "        # Print the loss after every epoch\n",
    "        if print_loss:\n",
    "            print('Epoch: {} | Loss: {}'.format(epoch, self.loss(X, Y)))\n",
    "\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        '''\n",
    "        Computes the predictions for a 2 layer NN given examples X and\n",
    "        stores them in self.v2.\n",
    "        Stores intermediate values before the prediction task in self.v1 and\n",
    "        self.a1\n",
    "        :param X: 2D Numpy array where each row contains an example.\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO:\n",
    "        pass\n",
    "\n",
    "    def backward_pass(self, X, Y):\n",
    "        '''\n",
    "        Computes the weights gradient and updates all four weights and bias gradients\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: Compute the gradients for the model's weights using backprop() method\n",
    "\n",
    "        # TODO: Update the weights using gradient_descent() method\n",
    "        pass\n",
    "\n",
    "    def backprop(self, X, Y):\n",
    "        '''\n",
    "        Computes the average weights and biases gradients for the given batch\n",
    "        :param X: 2D Numpy array where each row contains an example.\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: 4 Numpy arrays representing the computed gradients for each weight and bias\n",
    "        '''\n",
    "        # TODO: Call the \"get gradient\" methods\n",
    "        pass\n",
    "\n",
    "    def gradient_descent(self, grad_wh, grad_bh, grad_wout, grad_bout):\n",
    "        '''\n",
    "        Updates the weights using the given gradients\n",
    "        :param grad_wh: Numpy array representing the hidden weights gradient\n",
    "        :param grad_bh: Numpy array representing the hidden bias gradient\n",
    "        :param grad_wout: Numpy array representing the output weights gradient\n",
    "        :param grad_bout: Numpy array representing the output bias gradient\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: Update the weights using the given gradients and the learning rate\n",
    "        # Refer to the SGD algorithm in slide\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the total squared error on some dataset (X, Y).\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A float which is the squared error of the model on the dataset\n",
    "        '''\n",
    "        # Perform the forward pass and compute the l2 loss\n",
    "        self.forward_pass(X)\n",
    "        return l2_loss(self.v2, Y)\n",
    "\n",
    "    def average_loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the mean squared error on some dataset (X, Y).\n",
    "        MSE = Total squared error/# of examples\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A float which is the mean squared error of the model on the dataset\n",
    "        '''\n",
    "        return self.loss(X, Y) / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import pytest\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create Test Model for OneLayerNN\n",
    "test_model = OneLayerNN()\n",
    "\n",
    "# Creates Test Data\n",
    "x_bias = np.array([[0, 4, 1], [0, 3, 1], [5, 0, 1], [4, 1, 1], [0, 5, 1]])\n",
    "y = np.array([0, 0, 1, 1, 0])\n",
    "\n",
    "# Set Model Weights\n",
    "test_model.weights = np.array([[0.5488135, 0.71518937, 0.60276338]])\n",
    "\n",
    "# Test Model Forward Pass\n",
    "test_model.forward_pass(x_bias)\n",
    "assert test_model.v.shape == (1, 5), f\"Incorrect shape for v.\"\n",
    "assert test_model.v == pytest.approx(np.array(\n",
    "    [[3.46352086, 2.74833149, 3.34683088, 3.51320675, 4.17871023]]), .01), f\"Incorrect values for v.\"\n",
    "\n",
    "# Test Model Backward Pass\n",
    "grad_w = test_model.backprop(x_bias, y)\n",
    "assert grad_w.shape == (1, 3), f\"Incorrect shape for weights gradient.\"\n",
    "assert grad_w == pytest.approx(np.array(\n",
    "    [[8.71479256, 18.20233432, 6.10024008]]), .01), f\"Incorrect values for weights gradient.\"\n",
    "\n",
    "# Test Model Train and Check Model Weights\n",
    "test_model.train(x_bias, y, print_loss=False)\n",
    "assert test_model.weights.shape == (\n",
    "    1, 3), f\"Incorrect shape for layer one weights gradient.\"\n",
    "assert test_model.weights == pytest.approx(np.array(\n",
    "    [[0.17817953, -0.03543112, 0.34761945]]), .01), f\"Incorrect values for layer one weights gradient.\"\n",
    "\n",
    "print(\"Pass Test for OneLayerNN\")\n",
    "\n",
    "# Create Test Model for TwoLayerNN\n",
    "model = TwoLayerNN(2)\n",
    "\n",
    "# Create Test Data\n",
    "x = np.array([[15.0, -5.5]])\n",
    "y = np.array([2.0])\n",
    "\n",
    "# Set Model Weights\n",
    "model.wh = np.array([[-0.17795209, -0.0759435], [-0.01952383, 0.13401861]])\n",
    "model.bh = np.array([[0.], [0.]])\n",
    "model.wout = np.array([[-0.22206318, -0.17802104]])\n",
    "model.bout = np.array([[0.]])\n",
    "\n",
    "# Test Model Forward Pass\n",
    "model.forward_pass(x)\n",
    "assert model.a1.shape == (2, 1), f\"Incorrect shape for a1.\"\n",
    "assert model.a1 == pytest.approx(\n",
    "    np.array([[-2.2515921], [-1.02995981]]), .01), f\"Incorrect values for a1.\"\n",
    "assert model.v1.shape == (2, 1), f\"Incorrect shape for v1.\"\n",
    "assert model.v1 == pytest.approx(\n",
    "    np.array([[0.09521222], [0.2630919]]), .01), f\"Incorrect values for v1.\"\n",
    "assert model.a2.shape == (1, 1), f\"Incorrect shape for a2.\"\n",
    "assert model.a2 == pytest.approx(\n",
    "    np.array([[-0.06797902]]), .01), f\"Incorrect values for a2.\"\n",
    "assert model.v2.shape == (1, 1), f\"Incorrect shape for v2.\"\n",
    "assert model.v2 == pytest.approx(\n",
    "    np.array([[-0.06797902]]), .01), f\"Incorrect values for v2.\"\n",
    "\n",
    "# Test Model Backward Pass\n",
    "grad_wout = model._get_layer2_weights_gradient(x, y)\n",
    "assert grad_wout.shape == (\n",
    "    1, 2), f\"Incorrect shape for layer two weights gradient.\"\n",
    "assert grad_wout == pytest.approx(np.array(\n",
    "    [[-0.39379374, -1.08813702]]), .01), f\"Incorrect values for layer two weights gradient.\"\n",
    "grad_bout = model._get_layer2_bias_gradient(x, y)\n",
    "assert grad_bout.shape == (\n",
    "    1, 1), f\"Incorrect shape for layer two bias gradient.\"\n",
    "assert grad_bout == pytest.approx(np.array(\n",
    "    [[-4.13595804]]), .01), f\"Incorrect values for layer two bias gradient.\"\n",
    "grad_wh = model._get_layer1_weights_gradient(x, y)\n",
    "assert grad_wh.shape == (\n",
    "    2, 2), f\"Incorrect shape for layer one weights gradient.\"\n",
    "assert grad_wh == pytest.approx(np.array([[1.18681592, -0.43516584], [\n",
    "                                2.14121128, -0.7851108]]), .01), f\"Incorrect values for layer one weights gradient.\"\n",
    "grad_bh = model._get_layer1_bias_gradient(x, y)\n",
    "assert grad_bh.shape == (2, 1), f\"Incorrect shape for layer one bias gradient.\"\n",
    "assert grad_bh == pytest.approx(np.array(\n",
    "    [[0.07912106], [0.14274742]]), .01), f\"Incorrect values for layer one bias gradient.\"\n",
    "\n",
    "# Test Train Model and Check Model Weights\n",
    "model.train(x, y, print_loss=False)\n",
    "assert model.wh.shape == (2, 2), f\"Incorrect shape for layer one weights.\"\n",
    "assert model.wh == pytest.approx(np.array([[0.94962594, 0.66250674], [\n",
    "                                 0.32779044, 0.50763253]]), .01), f\"Incorrect values for layer one weights.\"\n",
    "assert model.bh.shape == (2, 1), f\"Incorrect shape for layer one bias.\"\n",
    "assert model.bh == pytest.approx(np.array(\n",
    "    [[3.65895295e-06], [2.09479202e-02]]), .01), f\"Incorrect values for layer one bias.\"\n",
    "assert model.wout.shape == (1, 2), f\"Incorrect shape for layer two weights.\"\n",
    "assert model.wout == pytest.approx(np.array(\n",
    "    [[0.86660131, 1.02218838]]), .01), f\"Incorrect values for layer two weights.\"\n",
    "assert model.bout.shape == (1, 1), f\"Incorrect shape for layer two bias.\"\n",
    "assert model.bout == pytest.approx(\n",
    "    np.array([[0.19294649]]), .01), f\"Incorrect values for layer two bias.\"\n",
    "\n",
    "print(\"Pass Test for TwoLayerNN\")\n",
    "\n",
    "# [TODO] Print your name and the date, using today function from date\n",
    "print(\"Name: Student Name\")\n",
    "print(\"Date:\", date.today())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def test_models(dataset, test_size=0.2):\n",
    "    '''\n",
    "        Tests LinearRegression, OneLayerNN, TwoLayerNN on a given dataset.\n",
    "        :param dataset The path to the dataset\n",
    "        :return None\n",
    "    '''\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(dataset):\n",
    "        print('The file {} does not exist'.format(dataset))\n",
    "        exit()\n",
    "\n",
    "    # Load in the dataset\n",
    "    data = np.loadtxt(dataset, skiprows = 1)\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    # Normalize the features\n",
    "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
    "\n",
    "    print('Running models on {} dataset'.format(dataset))\n",
    "\n",
    "    # Add a bias\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    #### 1-Layer NN ######\n",
    "    print('----- 1-Layer NN -----')\n",
    "    nnmodel = OneLayerNN()\n",
    "    nnmodel.train(X_train_b, Y_train, print_loss=False)\n",
    "    print('Average Training Loss:', nnmodel.average_loss(X_train_b, Y_train))\n",
    "    print('Average Testing Loss:', nnmodel.average_loss(X_test_b, Y_test))\n",
    "\n",
    "    #### 2-Layer NN ######\n",
    "    print('----- 2-Layer NN -----')\n",
    "    model = TwoLayerNN(10)\n",
    "    model.train(X_train, Y_train, print_loss=False)\n",
    "    print('Average Training Loss:', model.average_loss(X_train, Y_train))\n",
    "    print('Average Testing Loss:', model.average_loss(X_test, Y_test))\n",
    "\n",
    "\n",
    "# Set random seeds. DO NOT CHANGE THIS IN YOUR FINAL SUBMISSION.\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "# Uncomment to test gradient calculating functions for 2-layer NN\n",
    "test_models('wine.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Report**\n",
    "\n",
    "### **Question 1**\n",
    "Compare the average loss of the two models. Provide an explanation\n",
    "for what you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "Comment on your parameter choices. These include the learning rate,\n",
    "the hidden layer size and the number of epochs for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 3**\n",
    "Among machine learning techniques, neural networks have a reputation\n",
    "for being '[black boxes](https://ec.europa.eu/research-and-innovation/en/horizon-magazine/opening-black-box-artificial-intelligence)',\n",
    "where the logic of their decision making is difficult or impossible for humans to interpret.\n",
    "\n",
    "1.  If a 'black box' model gives an answer that disagrees with a\n",
    "    human expert, which answer should be believed? Are there\n",
    "    circumstances where we should believe one party more often than\n",
    "    the other?\n",
    "\n",
    "2.  Companies often hide the logic behind their products by making\n",
    "    their software closed-source. Are there differences between\n",
    "    companies selling closed source software, where the logic is\n",
    "    known but hidden, versus companies selling 'black box'\n",
    "    artificial intelligence software, where the logic might be\n",
    "    altogether unknown? Is using one type of software more\n",
    "    justifiable than using the other?\n",
    "    \n",
    "Please credit any outside sources you use in your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "1.\n",
    "\n",
    "2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
