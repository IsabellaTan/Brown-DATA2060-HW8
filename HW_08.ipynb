{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 8**\n",
    "\n",
    "Due: **November 26, 5pm** (late submission until November 29, 5pm -- no submission possible afterwards)\n",
    "\n",
    "Written assigment: 20 points\n",
    "\n",
    "Coding assignment: 25 points\n",
    "\n",
    "Project report: 10 points\n",
    "\n",
    "### Name: [Yawen Tan]\n",
    "\n",
    "### Link to the github repo: [https://github.com/IsabellaTan/Brown-DATA2060-HW8#]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Written Assignment**\n",
    "\n",
    "### **Neural Networks**\n",
    "Here, we consider a 2-layer neural network that takes inputs of\n",
    "dimension $d$, has a hidden layer of size $m$, and produces scalar\n",
    "outputs.\n",
    "\n",
    "The network's parameters are $W$, $\\mathbf{b_1}$, $\\mathbf{v}$, and $b_2$. $W$\n",
    "is a $m \\times d$ matrix, $\\mathbf{b_1}$ is an $m$-dimensional vector,\n",
    "$\\mathbf{v}$ is an $m$-dimensional vector, and $b_2$ is a scalar. <br>\n",
    "For an input $\\mathbf{x}$, the output of the first layer of the network is:\n",
    "$\\mathbf{h} = \\sigma(W\\mathbf{x}+\\mathbf{b_1})$\n",
    "and the output of the second layer is: $$z = \\mathbf{v}\\cdot \\mathbf{h} + b_2,$$ where $\\sigma$ is an activation\n",
    "function. For this question, let $\\sigma$ be the sigmoid activation\n",
    "function $\\sigma_{\\text{sigmoid}}$ (in the formula below, we apply it\n",
    "element-wise): $$\\sigma_{\\text{sigmoid}}(a) = \\frac{1}{1+e^{-a}}$$ We\n",
    "will be using the following loss function: $$L(z) = (z - y)^2,$$ where\n",
    "$y$ is a real-valued label and $z$ is the network's output. <br>\n",
    "In this problem, you will calculate the partial derivative of $L(z)$\n",
    "with respect to each of the network's parameters. Let $w_{ij}$ be the\n",
    "entry at the $i^{th}$ row and $j^{th}$ column of $W$. Let $v_i$ be the $i^{th}$\n",
    "component of $\\mathbf{v}$. Let $b_{1i}$ be the ith component of\n",
    "$\\mathbf{b_1}$. (Note that $1 \\leq i \\leq m$ and $1 \\leq j \\leq d$.) <br>\n",
    "(Hint: For each part of the problem, apply the chain rule.)\n",
    "\n",
    "### **Question 1**\n",
    "Calculate $\\frac{\\partial{L(z)}}{\\partial{b_2}}$. Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "$\\frac{\\partial{L(z)}}{\\partial{b_2}}$ = $\\frac{\\partial{L(z)}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{b_2}}$\n",
    "\n",
    "$\\frac{\\partial{L(z)}}{\\partial{z}}$ = $\\frac{\\partial{(z-y)^2}}{\\partial{z}}$ = 2(z-y)\n",
    "\n",
    "$\\frac{\\partial{z}}{\\partial{b_2}}$ = $\\frac{\\partial{(v\\cdot h + b_2)}}{\\partial{b_2}}$ = 1\n",
    "\n",
    "$\\frac{\\partial{L(z)}}{\\partial{b_2}}$ = $\\frac{\\partial{L(z)}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{b_2}}$ = 2(z-y) $\\cdot$ 1 = 2(z-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "Calculate $\\frac{\\partial{L(z)}}{\\partial{v_i}}$. Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "$\\frac{\\partial{L(z)}}{\\partial{v_i}}$ = $\\frac{\\partial{L(z)}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{v_i}}$\n",
    "\n",
    "From question 1, we already know $\\frac{\\partial{L(z)}}{\\partial{z}}$ = 2(z-y)\n",
    "\n",
    "Because $z = \\mathbf{v}\\cdot \\mathbf{h} + b_2$ where $\\mathbf{v}$ is an $m$-dimensional vector and $\\mathbf{h} = \\sigma(W\\mathbf{x}+\\mathbf{b_1})$, then z = $\\sum_{k=1}^{m} v_k \\cdot h_k + b_2$, so that $\\frac{\\partial{z}}{\\partial{v_i}}$ = $h_i$\n",
    "\n",
    "$\\frac{\\partial{L(z)}}{\\partial{v_i}}$ = $\\frac{\\partial{L(z)}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{v_i}}$ = 2(z-y) $\\cdot h_i$ = $2h_i(z-y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 3**\n",
    "Calculate $\\frac{\\partial{L(z)}}{\\partial{b_{1i}}}$. Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "$\\frac{\\partial{L(z)}}{\\partial{b_{1i}}}$ = $\\frac{\\partial{L}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{h_i}} \\cdot \\frac{\\partial{h_i}}{\\partial{a_i}} \\cdot \\frac{\\partial{a_i}}{\\partial{b_{1i}}}$\n",
    "\n",
    "$\\frac{\\partial{L(z)}}{\\partial{z}}$ = 2(z-y) from question 1\n",
    "\n",
    "$\\frac{\\partial{z}}{\\partial{h_i}}$ = $v_i$ because z = $\\sum_{k=1}^{m} v_k \\cdot h_k + b_2$\n",
    "\n",
    "Since $h_i = \\sigma_{\\text{sigmoid}}(a_i) = \\frac{1}{1+e^{-a_i}}$, then $\\frac{\\partial{h_i}}{\\partial{a_i}} = \\sigma(a_i) (1-\\sigma(a_i)) = h_i(1-h_i)$ \n",
    "\n",
    "Since $a_i = \\sum_{k=1}^{d} w_{ik} x_k + b_{1i}$, then $\\frac{\\partial{a_i}}{\\partial{b_{1i}}}$ = 1\n",
    "\n",
    "$\\frac{\\partial{L(z)}}{\\partial{b_{1i}}}$ = $\\frac{\\partial{L}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{h_i}} \\cdot \\frac{\\partial{h_i}}{\\partial{a_i}} \\cdot \\frac{\\partial{a_i}}{\\partial{b_{1i}}}$ = $2(z-y) \\cdot v_i \\cdot h_i(1-h_i) \\cdot 1$ = $2v_i(z-y) h_i(1-h_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 4**\n",
    "Calculate $\\frac{\\partial{L(z)}}{\\partial{w_{ij}}}$. Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "$\\frac{\\partial{L(z)}}{\\partial{w_{ij}}}$ = $\\frac{\\partial{L}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{h_i}} \\cdot \\frac{\\partial{h_i}}{\\partial{a_i}} \\cdot \\frac{\\partial{a_i}}{\\partial{w_{ij}}}$\n",
    "\n",
    "From question 3, we can get: \n",
    "\n",
    "$\\frac{\\partial{L(z)}}{\\partial{z}}$ = 2(z-y)\n",
    "\n",
    "$\\frac{\\partial{z}}{\\partial{h_i}}$ = $v_i$\n",
    "\n",
    "$\\frac{\\partial{h_i}}{\\partial{a_i}} = h_i(1-h_i)$ \n",
    "\n",
    "Since $a_i = \\sum_{k=1}^{d} w_{ik} x_k + b_{1i}$, then $\\frac{\\partial{a_i}}{\\partial{w_{ij}}}$ = $x_j$\n",
    "\n",
    "$\\frac{\\partial{L(z)}}{\\partial{w_{ij}}}$ = $\\frac{\\partial{L}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{h_i}} \\cdot \\frac{\\partial{h_i}}{\\partial{a_i}} \\cdot \\frac{\\partial{a_i}}{\\partial{w_{ij}}}$ = $2(z-y) \\cdot v_i \\cdot h_i(1-h_i) \\cdot x_j$ = $2v_i(z-y) h_i(1-h_i) x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Programming Assignment**\n",
    "\n",
    "### Introduction \n",
    "\n",
    "In this assignment, you will be implementing feed forward neural\n",
    "networks using stochastic gradient descent. You will implement two\n",
    "neural networks: a single layer neural network and a two-layer neural\n",
    "network. You will compare the performance of both models on the UCI Wine\n",
    "Dataset, which you previously used in HW2. The task is to predict the\n",
    "quality of a wine (scored out of 10) given various attributes of the\n",
    "wine (for example, acidity, alcohol content). The book section relevant\n",
    "to this assignment is 20.1.\n",
    "\n",
    "### Stencil Code & Data\n",
    "\n",
    "We have provided the following stencil code:\n",
    "\n",
    "-   `Models` contains the `OneLayerNN` model and the `TwoLayerNN`\n",
    "    model which you will be implementing.\n",
    "\n",
    "-   `Check Model` contains a series of tests to ensure you are coding your \n",
    "    model properly.\n",
    "\n",
    "-   `Main` is the entry point of program which will read in the\n",
    "    dataset, run the models and print the results.\n",
    "\n",
    "You should not need to add any code to `Main`. If you do for\n",
    "debugging or other purposes, please make sure all of your additions are\n",
    "commented out in the final handin. All the functions you need to fill in\n",
    "reside in `Models`, marked by `TODO`s.\n",
    "\n",
    "We have provided unit tests for the functions that compute gradients for\n",
    "the 2-layer neural network. These are `_layer1_weights_gradient`,\n",
    "`_layer1_bias_gradient`, `_layer2_weights_gradient`, and\n",
    "`_layer2_bias_gradient`. To enable these unit tests, uncomment\n",
    "`test_gradients` in the `main` function of `main.py`.\n",
    "\n",
    "Your program assumes the data is formatted as follows: The first column\n",
    "of data in each file is the dependent variable (the observations $y$)\n",
    "and all other columns are the independent input variables\n",
    "($x_{1}, x_{2}, \\ldots, x_{n}$). We have taken care of all data\n",
    "preprocessing, as usual.\n",
    "\n",
    "If you're curious and would like to read about the dataset, you can find\n",
    "more information [here](https://archive.ics.uci.edu/ml/datasets/wine), but it is\n",
    "strongly recommended that you use the versions that we've provided in\n",
    "the course directory to maintain consistent formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Assignment**\n",
    "\n",
    "### **Neural Networks**\n",
    "\n",
    "For this assignment, we will be evaluating each model using total\n",
    "squared loss (or L2 loss). Recall that the L2 loss function is defined\n",
    "as:\n",
    "\n",
    "$L(h) = \\sum\\limits_{i=1}^m \\ell(h({\\bf x}_i), y_i) = \\sum\\limits_{i=1}^m(y_{i}-h({\\bf x}_{i}))^{2}$\n",
    "\n",
    "where $y_{i}$ is the target value of $i^{th}$ sample and\n",
    "$h({\\bf x}_{i})$ is the predicted value given the learned model weights.\n",
    "Each of the two models will use stochastic gradient descent to minimize\n",
    "this loss function.\n",
    "\n",
    "For this assignment, you will be implementing two models:\n",
    "\n",
    "-   `OneLayerNN`: The one-layer neural network is an equivalent model to\n",
    "    Linear Regression. It also learns linear functions of the inputs:\n",
    "    $$h({\\bf x}) = \\langle {\\bf w},  {\\bf x} \\rangle + b.$$\n",
    "\n",
    "    Therefore, when using squared loss, the ERM hypothesis has weights\n",
    "    $${\\bf w} = \\text{argmin}_{{\\bf w}} \\sum_{i = 1}^{m}(y_{i} - h({\\bf x}_i))^{2} .$$\n",
    "\n",
    "    To find the optimal set of weights, you should use Stochastic\n",
    "    Gradient Descent. *Hint:* Compute the derivative of the loss with\n",
    "    respect to ${\\bf w}$. Then, use the SGD algorithm to minimize the\n",
    "    loss.\n",
    "\n",
    "-   `TwoLayerNN`: For this model, you will be implement a neural network\n",
    "    with a fully connected hidden layer.\n",
    "\n",
    "    For an input ${\\bf x}$, the output of the first layer of the network\n",
    "    is $${\\bf v} = \\sigma(W_1 {\\bf x}+{\\bf b}_1)$$ and the output of the\n",
    "    second layer is $$h = \\langle {\\bf w}_2, {\\bf v} \\rangle + b_2 ~.$$\n",
    "\n",
    "    $\\sigma$ is an activation function. In your implementation, you will\n",
    "    take in the activation function $\\sigma(a)$ as a parameter to\n",
    "    $\\texttt{TwoLayerNN}$. Additionally, you will need to pass in the\n",
    "    derivative of the activation function $\\sigma'(a)$ for training.\n",
    "    Doing so will allow you to easily swap out the sigmoid activation\n",
    "    function with other activation functions, such as ReLU. (You can\n",
    "    explore other activation functions for extra credit.)\n",
    "\n",
    "    To complete this assignment, however, you only need to train the\n",
    "    network with the sigmoid activation function. Recall that the\n",
    "    sigmoid activation function is (in the above formula, we apply it\n",
    "    element-wise),\n",
    "    $$\\sigma_{\\text{sigmoid}}(a) = \\frac{1}{1+e^{-a}} .$$\n",
    "     \n",
    "\n",
    "### **Training Neural Networks**\n",
    "\n",
    "The primary objective of training a neural network is to find a set of\n",
    "weights and biases that minimize the loss of our network, which in this\n",
    "case, is L2 loss. If these weights are all initialized to the same\n",
    "constant value, then they will all learn the same features. To avoid\n",
    "this, be sure that your implementation randomly initializes the weights.\n",
    "Numpy functions such as `np.random.normal` or `np.random.uniform` may be\n",
    "useful.\n",
    "\n",
    "When training the two layer neural network, first calculate the\n",
    "gradients of the weights and biases for both layers before updating\n",
    "them. In the stencil, we have given you four methods for computing\n",
    "gradients: `_layer1_weights_gradient`, `_layer1_bias_gradient`,\n",
    "`_layer2_weights_gradient`, and `_layer2_bias_gradient`. Each of these\n",
    "methods should be called before performing gradient descent, i.e. before\n",
    "updating all of the gradients.\n",
    "\n",
    "We also expect that you implement backpropagation as outlined in lecture\n",
    "i.e. computing all the outputs in the forward pass and saving them for\n",
    "use in the backward pass so that backpropagation achieves O(E)\n",
    "complexity, where E represents the number of edges in the network.\n",
    "\n",
    "### **Computing Gradients**\n",
    "\n",
    "Please refer to Lecture 17, slide 34 (Backpropagation) for the\n",
    "definition of the gradient computations.\n",
    "\n",
    "Remember that $\\sigma_1$ and $\\sigma_2$ are the activation functions at\n",
    "each layer, and not necessarily the same! Keep in mind that in your\n",
    "implementation for this assignment there should be no activation applied\n",
    "to the output layer of the network.\n",
    "\n",
    "Finally, note that the initial input matrix is comprised of rows, but\n",
    "the input to each gradient function is a vector. This is due to\n",
    "contradicting conventions about how to represent training data and\n",
    "neural network inputs. To resolve this, you may choose to reshape or\n",
    "transpose the input matrix somewhere in the train method.\n",
    "\n",
    "**Important Note:** External libraries that make the implementation\n",
    "trivial are prohibited. Specifically, `numpy.linalg.lstsqnp` (and\n",
    "similar functions) cannot be used in your implementation. Additionally,\n",
    "you **cannot** use Tensorflow or other neural network libraries. You\n",
    "should implement the neural networks using only Python and Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the evironment test below, make sure you get all green checks. If not, you will lose 2 points for each red or missing sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.11\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.10.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.7.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pytest version 8.4.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m torch version 2.7.1 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.11\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.11\"):\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.10.5\", 'numpy': \"2.3.2\",'sklearn': \"1.7.1\", \n",
    "                'pandas': \"2.3.2\", 'pytest': \"8.4.1\", 'torch':\"2.7.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def l2_loss(predictions,Y):\n",
    "    '''\n",
    "        Computes L2 loss (sum squared loss) between true values, Y, and predictions.\n",
    "        :param Y: A 1D Numpy array with real values (float64)\n",
    "        :param predictions: A 1D Numpy array of the same size of Y\n",
    "        :return: L2 loss using predictions for Y.\n",
    "    '''\n",
    "    # TODO\n",
    "    return np.sum((Y - predictions)**2)\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "        Sigmoid function f(x) =  1/(1 + exp(-x))\n",
    "        :param x: A scalar or Numpy array\n",
    "        :return: Sigmoid function evaluated at x (applied element-wise if it is an array)\n",
    "    '''\n",
    "    return np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (np.exp(x) + np.exp(0)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    '''\n",
    "        First derivative of the sigmoid function with respect to x.\n",
    "        :param x: A scalar or Numpy array\n",
    "        :return: Derivative of sigmoid evaluated at x (applied element-wise if it is an array)\n",
    "    '''\n",
    "    # TODO\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "class OneLayerNN:\n",
    "    '''\n",
    "        One layer neural network trained with Stocastic Gradient Descent (SGD)\n",
    "    '''\n",
    "    def __init__(self, batch_size=1):\n",
    "        '''\n",
    "        @attrs:\n",
    "            weights: The weights of the neural network model.\n",
    "            batch_size: The number of examples in each batch\n",
    "            learning_rate: The learning rate to use for SGD\n",
    "            epochs: The number of times to pass through the dataset\n",
    "            v: The resulting predictions computed during the forward pass\n",
    "        '''\n",
    "        # initialize self.weights in train()\n",
    "        self.weights = None\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = 0.001\n",
    "        self.epochs = 25\n",
    "\n",
    "        # initialize self.v in forward_pass()\n",
    "        self.v = None\n",
    "\n",
    "    def train(self, X, Y, print_loss=True):\n",
    "        '''\n",
    "        Trains the OneLayerNN model using SGD.\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :param print_loss: If True, print the loss after each epoch.\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: initialize weights\n",
    "        # HINT: for best performance initialize weights with np.random.uniform in the range [0, 1]\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.random.uniform(0, 1, size=(1, n_features))\n",
    "        # TODO: Train network for certain number of epochs\n",
    "        for epoch in range(self.epochs):\n",
    "            # TODO: Shuffle the examples (X) and labels (Y)\n",
    "            idx = np.random.permutation(n_samples)\n",
    "            X = X[idx]\n",
    "            Y = Y[idx]\n",
    "            # TODO: We need to iterate over each data point for each epoch\n",
    "            # iterate through the examples in batch size increments\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X[i:i+self.batch_size]\n",
    "                Y_batch = Y[i:i+self.batch_size]\n",
    "                # TODO: Perform the forward and backward pass on the current batch using forward_pass() and backward_pass() method\n",
    "                self.forward_pass(X_batch)\n",
    "                self.backward_pass(X_batch, Y_batch)\n",
    "            # Print the loss after every epoch\n",
    "            if print_loss:\n",
    "                print('Epoch: {} | Loss: {}'.format(epoch, self.loss(X, Y)))\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        '''\n",
    "        Computes the predictions for a single layer given examples X and\n",
    "        stores them in self.v\n",
    "        :param X: 2D Numpy array where each row contains an example.\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO:\n",
    "        self.v = self.weights @ X.T   # shape: (1, batch_size)\n",
    "\n",
    "    def backward_pass(self, X, Y):\n",
    "        '''\n",
    "        Computes the weights gradient and updates self.weights\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: Compute the gradients for the model's weights using backprop() method\n",
    "        grad = self.backprop(X, Y)  # shape (1, n_features)\n",
    "        # TODO: Update the weights using gradient_descent() method\n",
    "        self.gradient_descent(grad)\n",
    "\n",
    "    def backprop(self, X, Y):\n",
    "        '''\n",
    "        Returns the average weights gradient for the given batch\n",
    "        :param X: 2D Numpy array where each row contains an example.\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A 1D Numpy array representing the weights gradient\n",
    "        '''\n",
    "        # TODO: Compute the average weights gradient\n",
    "        # Refer to the SGD algorithm in slide\n",
    "        batch_size = X.shape[0]\n",
    "        preds = self.v.reshape(-1)\n",
    "        errors = preds - Y \n",
    "        # gradient = 2 * sum(errors[i] * X[i])\n",
    "        grad = 2 * (errors @ X) / batch_size\n",
    "        return grad.reshape(1, -1)\n",
    "\n",
    "    def gradient_descent(self, grad_W):\n",
    "        '''\n",
    "        Updates the weights using the given gradient\n",
    "        :param grad_W: A 1D Numpy array representing the weights gradient\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: Update the weights using the given gradient and the learning rate\n",
    "        # Refer to the SGD algorithm in slide\n",
    "        self.weights -= self.learning_rate * grad_W\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the total squared error on some dataset (X, Y).\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A float which is the squared error of the model on the dataset\n",
    "        '''\n",
    "        # Perform the forward pass and compute the l2 loss\n",
    "        self.forward_pass(X)\n",
    "        return l2_loss(self.v.flatten(), Y)\n",
    "\n",
    "    def average_loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the mean squared error on some dataset (X, Y).\n",
    "        MSE = Total squared error/# of examples\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A float which is the mean squared error of the model on the dataset\n",
    "        '''\n",
    "        return self.loss(X, Y) / X.shape[0]\n",
    "\n",
    "class TwoLayerNN:\n",
    "\n",
    "    def __init__(self, hidden_size, batch_size=1,\n",
    "                 activation=sigmoid, activation_derivative=sigmoid_derivative):\n",
    "        '''\n",
    "        @attrs:\n",
    "            activation: the activation function applied after the first layer\n",
    "            activation_derivative: the derivative of the activation function. Used for training.\n",
    "            hidden_size: The hidden size of the network (an integer)\n",
    "            batch_size: The number of examples in each batch\n",
    "            learning_rate: The learning rate to use for SGD\n",
    "            epochs: The number of times to pass through the dataset\n",
    "            wh: The first (hidden) layer weights of the neural network model.\n",
    "            bh: The first (hidden) layer bias of the neural network model.\n",
    "            wout: The second (output) layer weights of the neural network model.\n",
    "            bout: The second (output) layer bias of the neural network model.\n",
    "            a1: The output of the first layer computed during the forward pass\n",
    "            v1: The activated output of the first layer computed during the forward pass\n",
    "            a2: The output of the second layer computed during the forward pass\n",
    "            v2: The resulting predictions computed during the forward pass (layer 2 has the identity activation function)\n",
    "            output_neurons: The number of outputs of the network\n",
    "        '''\n",
    "        self.activation = activation\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = 0.01\n",
    "        self.epochs = 25\n",
    "\n",
    "        # initialize the following weights and biases in the train() method\n",
    "        self.wh = None\n",
    "        self.bh = None\n",
    "        self.wout = None\n",
    "        self.bout = None\n",
    "\n",
    "        # initialize the following values in the forward_pass() method\n",
    "        # these values will be stored and used for the backward_pass()\n",
    "        # note that you may not need to use them all in backward_pass()\n",
    "        self.a1 = None\n",
    "        self.v1 = None\n",
    "        self.a2 = None\n",
    "        self.v2 = None\n",
    "\n",
    "\n",
    "        # In this assignment, we will only use output_neurons = 1.\n",
    "        self.output_neurons = 1\n",
    "\n",
    "    def _get_layer2_bias_gradient(self, x, y):\n",
    "        '''\n",
    "        Computes the gradient of the loss with respect to the output bias, bout.\n",
    "        :param x: Numpy array for a single training example with dimension: input_size by 1\n",
    "        :param y: Label for the training example\n",
    "        :return: the partial derivates dL/dbout, a numpy array of dimension: output_neurons by 1\n",
    "        '''\n",
    "        # TODO:\n",
    "        h = self.v2.item()\n",
    "        grad = 2 * (h - y)\n",
    "        return grad.reshape(1,1)\n",
    "\n",
    "    def _get_layer2_weights_gradient(self, x, y):\n",
    "        '''\n",
    "        Computes the gradient of the loss with respect to the output weights, wout.\n",
    "        :param x: Numpy array for a single training example with dimension: input_size by 1\n",
    "        :param y: Label for the training example\n",
    "        :return: the partial derivates dL/dwout, a numpy array of dimension: output_neurons by hidden_size\n",
    "        '''\n",
    "        # TODO:\n",
    "        h_minus_y = 2 * (self.v2.item() - y)\n",
    "        return (h_minus_y * self.v1.T).reshape(1, self.hidden_size)\n",
    "\n",
    "    def _get_layer1_bias_gradient(self, x, y):\n",
    "        '''\n",
    "        Computes the gradient of the loss with respect to the hidden bias, bh.\n",
    "        :param x: Numpy array for a single training example with dimension: input_size by 1\n",
    "        :param y: Label for the training example\n",
    "        :return: the partial derivates dL/dbh, a numpy array of dimension: hidden_size by 1\n",
    "        '''\n",
    "        # TODO:\n",
    "        h_minus_y = 2 * (self.v2.item() - y)\n",
    "        delta = (self.wout.T * h_minus_y) * self.activation_derivative(self.a1)\n",
    "        return delta.reshape(self.hidden_size, 1)\n",
    "\n",
    "    def _get_layer1_weights_gradient(self, x, y):\n",
    "        '''\n",
    "        Computes the gradient of the loss with respect to the hidden weights, wh.\n",
    "        :param x: Numpy array for a single training example with dimension: input_size by 1\n",
    "        :param y: Label for the training example\n",
    "        :return: the partial derivates dL/dwh, a numpy array of dimension: hidden_size by input_size\n",
    "        '''\n",
    "        # TODO:\n",
    "        grad_b1 = self._get_layer1_bias_gradient(x, y)\n",
    "        return grad_b1 @ x.reshape(1, -1)\n",
    "\n",
    "    def train(self, X, Y, print_loss=True):\n",
    "        '''\n",
    "        Trains the TwoLayerNN with SGD using Backpropagation.\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :param learning_rate: The learning rate to use for SGD\n",
    "        :param epochs: The number of times to pass through the dataset\n",
    "        :param print_loss: If True, print the loss after each epoch.\n",
    "        :return: None\n",
    "        '''\n",
    "        # NOTE:\n",
    "        # Use numpy arrays of the following dimensions for your model's parameters.\n",
    "        # layer 1 weights (wh): hidden_size x input_size\n",
    "        # layer 1 bias (bh): hidden_size x 1\n",
    "        # layer 2 weights (wout): output_neurons x hidden_size\n",
    "        # layer 2 bias (bout): output_neurons x 1\n",
    "        # HINT: for best performance initialize weights with np.random.uniform in the range [0, 1]\n",
    "\n",
    "        # TODO: Weight and bias initialization\n",
    "        n_samples, input_dim = X.shape\n",
    "        self.wh = np.random.uniform(0, 1, (self.hidden_size, input_dim))\n",
    "        self.bh = np.zeros((self.hidden_size, 1))\n",
    "        self.wout = np.random.uniform(0, 1, (self.output_neurons, self.hidden_size))\n",
    "        self.bout = np.zeros((self.output_neurons, 1))\n",
    "\n",
    "        # TODO: Train network for certain number of epochs\n",
    "        for epoch in range(self.epochs):\n",
    "            # TODO: Shuffle the examples (X) and labels (Y)\n",
    "            idx = np.random.permutation(n_samples)\n",
    "            X = X[idx]\n",
    "            Y = Y[idx]\n",
    "            # TODO: We need to iterate over each data point for each epoch\n",
    "            # iterate through the examples in batch size increments\n",
    "            for i in range(n_samples):\n",
    "                x = X[i].reshape(-1, 1)   # column vector\n",
    "                y = Y[i]\n",
    "                # TODO: Perform the forward and backward pass on the current batch using forward_pass() and backward_pass() method\n",
    "                self.forward_pass(x.T)    # store a1, v1, a2, v2\n",
    "                self.backward_pass(x, y)\n",
    "            # Print the loss after every epoch\n",
    "            if print_loss:\n",
    "                print('Epoch: {} | Loss: {}'.format(epoch, self.loss(X, Y)))\n",
    "\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        '''\n",
    "        Computes the predictions for a 2 layer NN given examples X and\n",
    "        stores them in self.v2.\n",
    "        Stores intermediate values before the prediction task in self.v1 and\n",
    "        self.a1\n",
    "        :param X: 2D Numpy array where each row contains an example.\n",
    "        :return: None\n",
    "        '''\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        x = X.T\n",
    "        self.a1 = self.wh @ x + self.bh\n",
    "        self.v1 = self.activation(self.a1)\n",
    "        self.a2 = self.wout @ self.v1 + self.bout\n",
    "        self.v2 = self.a2\n",
    "\n",
    "    def backward_pass(self, X, Y):\n",
    "        '''\n",
    "        Computes the weights gradient and updates all four weights and bias gradients\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: Compute the gradients for the model's weights using backprop() method\n",
    "        grad_wout = self._get_layer2_weights_gradient(X, Y)\n",
    "        grad_bout = self._get_layer2_bias_gradient(X, Y)\n",
    "        grad_wh = self._get_layer1_weights_gradient(X, Y)\n",
    "        grad_bh = self._get_layer1_bias_gradient(X, Y)\n",
    "        # TODO: Update the weights using gradient_descent() method\n",
    "        self.gradient_descent(grad_wh, grad_bh, grad_wout, grad_bout)\n",
    "        \n",
    "\n",
    "    def backprop(self, X, Y):\n",
    "        '''\n",
    "        Computes the average weights and biases gradients for the given batch\n",
    "        :param X: 2D Numpy array where each row contains an example.\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: 4 Numpy arrays representing the computed gradients for each weight and bias\n",
    "        '''\n",
    "        # TODO: Call the \"get gradient\" methods\n",
    "        batch_size = X.shape[0]\n",
    "        grad_wh_total = np.zeros_like(self.wh)\n",
    "        grad_bh_total = np.zeros_like(self.bh)\n",
    "        grad_wout_total = np.zeros_like(self.wout)\n",
    "        grad_bout_total = np.zeros_like(self.bout)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            x = X[i].reshape(-1, 1)\n",
    "            y = Y[i]\n",
    "\n",
    "            self.forward_pass(x.T)\n",
    "\n",
    "            grad_wh_total += self._get_layer1_weights_gradient(x, y)\n",
    "            grad_bh_total += self._get_layer1_bias_gradient(x, y)\n",
    "            grad_wout_total += self._get_layer2_weights_gradient(x, y)\n",
    "            grad_bout_total += self._get_layer2_bias_gradient(x, y)\n",
    "\n",
    "        return (grad_wh_total / batch_size,\n",
    "                grad_bh_total / batch_size,\n",
    "                grad_wout_total / batch_size,\n",
    "                grad_bout_total / batch_size)\n",
    "\n",
    "    def gradient_descent(self, grad_wh, grad_bh, grad_wout, grad_bout):\n",
    "        '''\n",
    "        Updates the weights using the given gradients\n",
    "        :param grad_wh: Numpy array representing the hidden weights gradient\n",
    "        :param grad_bh: Numpy array representing the hidden bias gradient\n",
    "        :param grad_wout: Numpy array representing the output weights gradient\n",
    "        :param grad_bout: Numpy array representing the output bias gradient\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: Update the weights using the given gradients and the learning rate\n",
    "        # Refer to the SGD algorithm in slide\n",
    "        self.wh -= self.learning_rate * grad_wh\n",
    "        self.bh -= self.learning_rate * grad_bh\n",
    "        self.wout -= self.learning_rate * grad_wout\n",
    "        self.bout -= self.learning_rate * grad_bout\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the total squared error on some dataset (X, Y).\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A float which is the squared error of the model on the dataset\n",
    "        '''\n",
    "        # Perform the forward pass and compute the l2 loss\n",
    "        self.forward_pass(X)\n",
    "        return l2_loss(self.v2, Y)\n",
    "\n",
    "    def average_loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the mean squared error on some dataset (X, Y).\n",
    "        MSE = Total squared error/# of examples\n",
    "        :param X: 2D Numpy array where each row contains an example\n",
    "        :param Y: 1D Numpy array containing the corresponding values for each example\n",
    "        :return: A float which is the mean squared error of the model on the dataset\n",
    "        '''\n",
    "        return self.loss(X, Y) / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass Test for OneLayerNN\n",
      "Pass Test for TwoLayerNN\n",
      "Name: Yawen Tan\n",
      "Date: 2025-11-24\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "import pytest\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create Test Model for OneLayerNN\n",
    "test_model = OneLayerNN()\n",
    "\n",
    "# Creates Test Data\n",
    "x_bias = np.array([[0, 4, 1], [0, 3, 1], [5, 0, 1], [4, 1, 1], [0, 5, 1]])\n",
    "y = np.array([0, 0, 1, 1, 0])\n",
    "\n",
    "# Set Model Weights\n",
    "test_model.weights = np.array([[0.5488135, 0.71518937, 0.60276338]])\n",
    "\n",
    "# Test Model Forward Pass\n",
    "test_model.forward_pass(x_bias)\n",
    "assert test_model.v.shape == (1, 5), f\"Incorrect shape for v.\"\n",
    "assert test_model.v == pytest.approx(np.array(\n",
    "    [[3.46352086, 2.74833149, 3.34683088, 3.51320675, 4.17871023]]), .01), f\"Incorrect values for v.\"\n",
    "\n",
    "# Test Model Backward Pass\n",
    "grad_w = test_model.backprop(x_bias, y)\n",
    "assert grad_w.shape == (1, 3), f\"Incorrect shape for weights gradient.\"\n",
    "assert grad_w == pytest.approx(np.array(\n",
    "    [[8.71479256, 18.20233432, 6.10024008]]), .01), f\"Incorrect values for weights gradient.\"\n",
    "\n",
    "# Test Model Train and Check Model Weights\n",
    "test_model.train(x_bias, y, print_loss=False)\n",
    "assert test_model.weights.shape == (\n",
    "    1, 3), f\"Incorrect shape for layer one weights gradient.\"\n",
    "assert test_model.weights == pytest.approx(np.array(\n",
    "    [[0.17817953, -0.03543112, 0.34761945]]), .01), f\"Incorrect values for layer one weights gradient.\"\n",
    "\n",
    "print(\"Pass Test for OneLayerNN\")\n",
    "\n",
    "# Create Test Model for TwoLayerNN\n",
    "model = TwoLayerNN(2)\n",
    "\n",
    "# Create Test Data\n",
    "x = np.array([[15.0, -5.5]])\n",
    "y = np.array([2.0])\n",
    "\n",
    "# Set Model Weights\n",
    "model.wh = np.array([[-0.17795209, -0.0759435], [-0.01952383, 0.13401861]])\n",
    "model.bh = np.array([[0.], [0.]])\n",
    "model.wout = np.array([[-0.22206318, -0.17802104]])\n",
    "model.bout = np.array([[0.]])\n",
    "\n",
    "# Test Model Forward Pass\n",
    "model.forward_pass(x)\n",
    "assert model.a1.shape == (2, 1), f\"Incorrect shape for a1.\"\n",
    "assert model.a1 == pytest.approx(\n",
    "    np.array([[-2.2515921], [-1.02995981]]), .01), f\"Incorrect values for a1.\"\n",
    "assert model.v1.shape == (2, 1), f\"Incorrect shape for v1.\"\n",
    "assert model.v1 == pytest.approx(\n",
    "    np.array([[0.09521222], [0.2630919]]), .01), f\"Incorrect values for v1.\"\n",
    "assert model.a2.shape == (1, 1), f\"Incorrect shape for a2.\"\n",
    "assert model.a2 == pytest.approx(\n",
    "    np.array([[-0.06797902]]), .01), f\"Incorrect values for a2.\"\n",
    "assert model.v2.shape == (1, 1), f\"Incorrect shape for v2.\"\n",
    "assert model.v2 == pytest.approx(\n",
    "    np.array([[-0.06797902]]), .01), f\"Incorrect values for v2.\"\n",
    "\n",
    "# Test Model Backward Pass\n",
    "grad_wout = model._get_layer2_weights_gradient(x, y)\n",
    "assert grad_wout.shape == (\n",
    "    1, 2), f\"Incorrect shape for layer two weights gradient.\"\n",
    "assert grad_wout == pytest.approx(np.array(\n",
    "    [[-0.39379374, -1.08813702]]), .01), f\"Incorrect values for layer two weights gradient.\"\n",
    "grad_bout = model._get_layer2_bias_gradient(x, y)\n",
    "assert grad_bout.shape == (\n",
    "    1, 1), f\"Incorrect shape for layer two bias gradient.\"\n",
    "assert grad_bout == pytest.approx(np.array(\n",
    "    [[-4.13595804]]), .01), f\"Incorrect values for layer two bias gradient.\"\n",
    "grad_wh = model._get_layer1_weights_gradient(x, y)\n",
    "assert grad_wh.shape == (\n",
    "    2, 2), f\"Incorrect shape for layer one weights gradient.\"\n",
    "assert grad_wh == pytest.approx(np.array([[1.18681592, -0.43516584], [\n",
    "                                2.14121128, -0.7851108]]), .01), f\"Incorrect values for layer one weights gradient.\"\n",
    "grad_bh = model._get_layer1_bias_gradient(x, y)\n",
    "assert grad_bh.shape == (2, 1), f\"Incorrect shape for layer one bias gradient.\"\n",
    "assert grad_bh == pytest.approx(np.array(\n",
    "    [[0.07912106], [0.14274742]]), .01), f\"Incorrect values for layer one bias gradient.\"\n",
    "\n",
    "# Test Train Model and Check Model Weights\n",
    "model.train(x, y, print_loss=False)\n",
    "assert model.wh.shape == (2, 2), f\"Incorrect shape for layer one weights.\"\n",
    "assert model.wh == pytest.approx(np.array([[0.94962594, 0.66250674], [\n",
    "                                 0.32779044, 0.50763253]]), .01), f\"Incorrect values for layer one weights.\"\n",
    "assert model.bh.shape == (2, 1), f\"Incorrect shape for layer one bias.\"\n",
    "assert model.bh == pytest.approx(np.array(\n",
    "    [[3.65895295e-06], [2.09479202e-02]]), .01), f\"Incorrect values for layer one bias.\"\n",
    "assert model.wout.shape == (1, 2), f\"Incorrect shape for layer two weights.\"\n",
    "assert model.wout == pytest.approx(np.array(\n",
    "    [[0.86660131, 1.02218838]]), .01), f\"Incorrect values for layer two weights.\"\n",
    "assert model.bout.shape == (1, 1), f\"Incorrect shape for layer two bias.\"\n",
    "assert model.bout == pytest.approx(\n",
    "    np.array([[0.19294649]]), .01), f\"Incorrect values for layer two bias.\"\n",
    "\n",
    "print(\"Pass Test for TwoLayerNN\")\n",
    "\n",
    "# [TODO] Print your name and the date, using today function from date\n",
    "print(\"Name: Yawen Tan\")\n",
    "print(\"Date:\", date.today())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on wine.txt dataset\n",
      "----- 1-Layer NN -----\n",
      "Average Training Loss: 0.5456218407141012\n",
      "Average Testing Loss: 0.6635895907223636\n",
      "----- 2-Layer NN -----\n",
      "Average Training Loss: 0.5123357911205262\n",
      "Average Testing Loss: 0.6376984561204923\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def test_models(dataset, test_size=0.2):\n",
    "    '''\n",
    "        Tests LinearRegression, OneLayerNN, TwoLayerNN on a given dataset.\n",
    "        :param dataset The path to the dataset\n",
    "        :return None\n",
    "    '''\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(dataset):\n",
    "        print('The file {} does not exist'.format(dataset))\n",
    "        exit()\n",
    "\n",
    "    # Load in the dataset\n",
    "    data = np.loadtxt(dataset, skiprows = 1)\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    # Normalize the features\n",
    "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
    "\n",
    "    print('Running models on {} dataset'.format(dataset))\n",
    "\n",
    "    # Add a bias\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    #### 1-Layer NN ######\n",
    "    print('----- 1-Layer NN -----')\n",
    "    nnmodel = OneLayerNN()\n",
    "    nnmodel.train(X_train_b, Y_train, print_loss=False)\n",
    "    print('Average Training Loss:', nnmodel.average_loss(X_train_b, Y_train))\n",
    "    print('Average Testing Loss:', nnmodel.average_loss(X_test_b, Y_test))\n",
    "\n",
    "    #### 2-Layer NN ######\n",
    "    print('----- 2-Layer NN -----')\n",
    "    model = TwoLayerNN(10)\n",
    "    model.train(X_train, Y_train, print_loss=False)\n",
    "    print('Average Training Loss:', model.average_loss(X_train, Y_train))\n",
    "    print('Average Testing Loss:', model.average_loss(X_test, Y_test))\n",
    "\n",
    "\n",
    "# Set random seeds. DO NOT CHANGE THIS IN YOUR FINAL SUBMISSION.\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "# Uncomment to test gradient calculating functions for 2-layer NN\n",
    "test_models('wine.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Report**\n",
    "\n",
    "### **Question 1**\n",
    "Compare the average loss of the two models. Provide an explanation\n",
    "for what you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "When comparing the two models, the two-layer neural network performs slightly better than the one-layer model. Its average loss is lower on both the training set and the testing set. This happens because the two-layer network has a hidden layer, which gives it more flexibility and allows it to capture patterns that arenâ€™t purely linear. In other words, the data likely contains some nonlinear structure, and the extra layer helps the model represent it more effectively. Also, the gap between training and testing loss is similar for both models, which means the two-layer network does not appear to be overfitting. Instead, it learns a slightly better representation of the data while still generalizing well to unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "Comment on your parameter choices. These include the learning rate,\n",
    "the hidden layer size and the number of epochs for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "The learning rate controls how large a step the model takes when updating its weights during training. A higher learning rate makes training faster but less stable, while a lower learning rate produces slower but more controlled updates.\n",
    "In the code, the one-layer network uses a learning rate of 0.001 and the two-layer network uses 0.01. Both values are small enough to keep training stable, and the slightly larger rate for the two-layer model helps it learn efficiently despite having more parameters.\n",
    "\n",
    "The hidden layer size determines how many units the network has to capture nonlinear relationships in the data. A larger hidden layer increases model capacity, while a smaller one keeps the network simpler and reduces the risk of overfitting. In the code, the two-layer model uses 10 hidden units. This gives the network enough flexibility to learn nonlinear structure in the wine dataset without making the model unnecessarily large.\n",
    "\n",
    "The number of epochs specifies how many full passes the model makes over the training data. More epochs allow the model to refine its weights, but too many can lead to overfitting, especially on small datasets. In the code, both models run for 25 epochs, which is enough for the loss to decrease and stabilize without showing obvious signs of overfitting.\n",
    "\n",
    "The batch size defines how many samples are used to compute each gradient update. A batch size of 1 corresponds to stochastic gradient descent, where the model updates after every example. This adds noise to the updates but often helps avoid poor local minima. In the code, both networks use a batch size of 1. This fits the small wine dataset and allows frequent updates without the overhead of computing gradients on larger batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 3**\n",
    "Among machine learning techniques, neural networks have a reputation\n",
    "for being '[black boxes](https://ec.europa.eu/research-and-innovation/en/horizon-magazine/opening-black-box-artificial-intelligence)',\n",
    "where the logic of their decision making is difficult or impossible for humans to interpret.\n",
    "\n",
    "1.  If a 'black box' model gives an answer that disagrees with a\n",
    "    human expert, which answer should be believed? Are there\n",
    "    circumstances where we should believe one party more often than\n",
    "    the other?\n",
    "\n",
    "2.  Companies often hide the logic behind their products by making\n",
    "    their software closed-source. Are there differences between\n",
    "    companies selling closed source software, where the logic is\n",
    "    known but hidden, versus companies selling 'black box'\n",
    "    artificial intelligence software, where the logic might be\n",
    "    altogether unknown? Is using one type of software more\n",
    "    justifiable than using the other?\n",
    "    \n",
    "Please credit any outside sources you use in your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "1. When a 'black-box' model gives an answer that disagrees with a human expert, there isnâ€™t a single rule that always tells us who is right. The better choice depends on the situation. In high-risk areasâ€”such as medicine, law, or financeâ€”we usually have stronger reasons to trust the expert. These fields rely heavily on domain knowledge, causal understanding, and real-world experience, which machine learning models may not fully capture. Models can also make mistakes when the input data is very different from their training data. On the other hand, there are cases where the model may deserve more weight. When the problem involves large amounts of data or subtle patterns that humans cannot easily detectâ€”such as image recognition or large-scale predictionsâ€”well-validated models can outperform experts. If the model has been tested rigorously, shows consistent accuracy, and operates within the same type of data it was trained on, its predictions can sometimes be more reliable than human judgment.\n",
    "\n",
    "2. There is an important difference between traditional closed-source software and black-box AI systems. With closed-source software, the logic is hidden from users, but it still exists in a clear, human-written form. Developers know exactly how each part of the system works, and in principle, the rules can be reviewed, audited, or explained if access is granted. Black-box AI models are different. Even the creators may not fully understand how the model arrives at specific decisions because the internal patterns are learned automatically from data, not designed by hand. This creates extra uncertainty: we may not know what features the model relies on, how it generalizes, or why it behaves unexpectedly in certain cases. Because of this, using closed-source software is usually easier to justify. Even though the code is hidden, there is still a clear and deterministic logic behind it. Black-box AI, on the other hand, may require additional safeguards, such as transparency reports, auditing, testing for bias, and human oversight, before it is used in important decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
